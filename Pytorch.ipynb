{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "a=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53720681, 0.19566059],\n",
       "       [0.76552398, 0.07939125]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9417, 0.2292],\n",
       "        [0.0063, 0.6658]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5488135 , 0.71518937],\n",
       "       [0.60276338, 0.54488318]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "np.random.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5488135 , 0.71518937],\n",
       "       [0.60276338, 0.54488318]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "np.random.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14675589, 0.09233859],\n",
       "       [0.18626021, 0.34556073]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4963, 0.7682],\n",
       "        [0.0885, 0.1320]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4963, 0.7682],\n",
       "        [0.0885, 0.1320]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3074, 0.6341],\n",
       "        [0.4901, 0.8964]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(1)\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "a=np.ones((2,2))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#Numpy to torch\n",
    "torch_a=torch.from_numpy(a)\n",
    "print(torch_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[[[1],\n",
      "          [1]],\n",
      "\n",
      "         [[1],\n",
      "          [1]],\n",
      "\n",
      "         [[1],\n",
      "          [1]]],\n",
      "\n",
      "\n",
      "        [[[1],\n",
      "          [1]],\n",
      "\n",
      "         [[1],\n",
      "          [1]],\n",
      "\n",
      "         [[1],\n",
      "          [1]]]], dtype=torch.int32)\n",
      "tensor([[1, 1],\n",
      "        [1, 1]], dtype=torch.uint8)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a=np.ones((2,2),dtype=np.int64)\n",
    "b=np.ones((2,3,2,1),dtype=np.int32)\n",
    "c=np.ones((2,2),dtype=np.uint8)\n",
    "d=np.ones((2,2),dtype=np.float64)\n",
    "print(torch.from_numpy(a))\n",
    "print(torch.from_numpy(b))\n",
    "print(torch.from_numpy(c))\n",
    "print(torch.from_numpy(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Torch to Numpy\n",
    "torch_a=torch.ones(2,2)\n",
    "print(type(torch_a))\n",
    "a=torch_a.numpy()\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor on CPU\n",
    "tcpu=torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU to GPU\n",
    "if torch.cuda.is_available():\n",
    "    tcpu.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcpu.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(tcpu.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b=tcpu.numpy()\n",
    "print(torch.from_numpy(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcpu.view(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(2,2)\n",
    "b=torch.ones(2,2)\n",
    "c=a+b\n",
    "print(torch.add(a,b))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.Tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=Variable(torch.ones(2,2),requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "b=Variable(torch.ones(2,2),requires_grad=True)\n",
    "print(torch.add(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Variable(torch.ones(2),requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(x.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20., 20.], grad_fn=<MulBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=5*(x+1)**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20., grad_fn=<MulBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o=(1/2)*torch.sum(y)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n=50\n",
    "x=np.random.randn(n)\n",
    "y=x*np.random.randn(n)\n",
    "\n",
    "colors=np.random.randn(n)\n",
    "plt.plot(np.unique(x),np.poly1d(np.polyfit(x,y,1))(np.unique(x)))\n",
    "plt.scatter(x,y,c=colors,alpha=0.5)\n",
    "plt.show()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_value=[i for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.array(x_value,dtype=np.float32)\n",
    "print(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train.reshape(-1,1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[2*i+1 for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1)\n"
     ]
    }
   ],
   "source": [
    "y=np.array(y,dtype=np.float32)\n",
    "print(y.shape)\n",
    "y=y.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(linearRegression,self).__init__()\n",
    "        self.linear=nn.Linear(input_dim,output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=1\n",
    "output_dim=1\n",
    "model=linearRegression(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 166.1709442138672\n",
      "epoch 2 loss 13.693824768066406\n",
      "epoch 3 loss 1.2551829814910889\n",
      "epoch 4 loss 0.2390594333410263\n",
      "epoch 5 loss 0.15465064346790314\n",
      "epoch 6 loss 0.14625655114650726\n",
      "epoch 7 loss 0.14407965540885925\n",
      "epoch 8 loss 0.14242620766162872\n",
      "epoch 9 loss 0.14083202183246613\n",
      "epoch 10 loss 0.13925911486148834\n",
      "epoch 11 loss 0.13770410418510437\n",
      "epoch 12 loss 0.13616636395454407\n",
      "epoch 13 loss 0.13464584946632385\n",
      "epoch 14 loss 0.13314227759838104\n",
      "epoch 15 loss 0.13165564835071564\n",
      "epoch 16 loss 0.13018526136875153\n",
      "epoch 17 loss 0.12873175740242004\n",
      "epoch 18 loss 0.12729401886463165\n",
      "epoch 19 loss 0.12587259709835052\n",
      "epoch 20 loss 0.1244669258594513\n",
      "epoch 21 loss 0.12307718396186829\n",
      "epoch 22 loss 0.12170261144638062\n",
      "epoch 23 loss 0.12034362554550171\n",
      "epoch 24 loss 0.11899986863136292\n",
      "epoch 25 loss 0.1176709309220314\n",
      "epoch 26 loss 0.11635694652795792\n",
      "epoch 27 loss 0.115057572722435\n",
      "epoch 28 loss 0.11377275735139847\n",
      "epoch 29 loss 0.11250225454568863\n",
      "epoch 30 loss 0.11124596744775772\n",
      "epoch 31 loss 0.11000378429889679\n",
      "epoch 32 loss 0.1087753176689148\n",
      "epoch 33 loss 0.10756061226129532\n",
      "epoch 34 loss 0.10635945945978165\n",
      "epoch 35 loss 0.10517176240682602\n",
      "epoch 36 loss 0.10399740189313889\n",
      "epoch 37 loss 0.10283605754375458\n",
      "epoch 38 loss 0.1016877144575119\n",
      "epoch 39 loss 0.10055210441350937\n",
      "epoch 40 loss 0.09942919760942459\n",
      "epoch 41 loss 0.09831899404525757\n",
      "epoch 42 loss 0.09722106158733368\n",
      "epoch 43 loss 0.09613539278507233\n",
      "epoch 44 loss 0.09506195783615112\n",
      "epoch 45 loss 0.09400039911270142\n",
      "epoch 46 loss 0.0929507464170456\n",
      "epoch 47 loss 0.09191278368234634\n",
      "epoch 48 loss 0.09088636189699173\n",
      "epoch 49 loss 0.08987144380807877\n",
      "epoch 50 loss 0.08886797726154327\n",
      "epoch 51 loss 0.08787557482719421\n",
      "epoch 52 loss 0.08689405024051666\n",
      "epoch 53 loss 0.08592384308576584\n",
      "epoch 54 loss 0.0849643349647522\n",
      "epoch 55 loss 0.0840156301856041\n",
      "epoch 56 loss 0.08307734876871109\n",
      "epoch 57 loss 0.08214971423149109\n",
      "epoch 58 loss 0.08123227208852768\n",
      "epoch 59 loss 0.08032521605491638\n",
      "epoch 60 loss 0.07942834496498108\n",
      "epoch 61 loss 0.07854127138853073\n",
      "epoch 62 loss 0.07766412198543549\n",
      "epoch 63 loss 0.07679705321788788\n",
      "epoch 64 loss 0.07593927532434464\n",
      "epoch 65 loss 0.07509143650531769\n",
      "epoch 66 loss 0.07425270974636078\n",
      "epoch 67 loss 0.07342364639043808\n",
      "epoch 68 loss 0.07260370254516602\n",
      "epoch 69 loss 0.07179302722215652\n",
      "epoch 70 loss 0.07099127769470215\n",
      "epoch 71 loss 0.07019852101802826\n",
      "epoch 72 loss 0.06941460072994232\n",
      "epoch 73 loss 0.06863953918218613\n",
      "epoch 74 loss 0.06787306070327759\n",
      "epoch 75 loss 0.06711502373218536\n",
      "epoch 76 loss 0.06636558473110199\n",
      "epoch 77 loss 0.06562460958957672\n",
      "epoch 78 loss 0.06489159166812897\n",
      "epoch 79 loss 0.06416705995798111\n",
      "epoch 80 loss 0.06345044821500778\n",
      "epoch 81 loss 0.06274204701185226\n",
      "epoch 82 loss 0.062041301280260086\n",
      "epoch 83 loss 0.06134850159287453\n",
      "epoch 84 loss 0.060663461685180664\n",
      "epoch 85 loss 0.05998610705137253\n",
      "epoch 86 loss 0.05931618809700012\n",
      "epoch 87 loss 0.0586538165807724\n",
      "epoch 88 loss 0.05799887701869011\n",
      "epoch 89 loss 0.05735111981630325\n",
      "epoch 90 loss 0.05671066418290138\n",
      "epoch 91 loss 0.05607745051383972\n",
      "epoch 92 loss 0.05545135587453842\n",
      "epoch 93 loss 0.05483207106590271\n",
      "epoch 94 loss 0.05421983450651169\n",
      "epoch 95 loss 0.05361425504088402\n",
      "epoch 96 loss 0.05301562696695328\n",
      "epoch 97 loss 0.052423518151044846\n",
      "epoch 98 loss 0.0518382228910923\n",
      "epoch 99 loss 0.05125928670167923\n",
      "epoch 100 loss 0.050686854869127274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    epoch+=1\n",
    "    inputs=Variable(torch.from_numpy(X_train))\n",
    "    labels=Variable(torch.from_numpy(y))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs=model(inputs)\n",
    "    loss=criteria(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch {} loss {}'.format(epoch,loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset=dsets.MNIST(root='./data',\n",
    "                         train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xc08f76b5f8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_dataset[0][0].numpy().reshape(28,28),'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=dsets.MNIST(root='./data',\n",
    "                        train=False,\n",
    "                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xc08f7984e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADQNJREFUeJzt3W+MVfWdx/HPZylNjPQBWLHEgnQb3bgaAzoaE3AzamxYbYKN1NQHGzbZMH2AZps0ZA1PypMmjemfrU9IpikpJtSWhFbRGBeDGylRGwejBYpQICzMgkAzJgUT0yDfPphDO8W5v3u5/84dv+9XQube8z1/vrnhM+ecOefcnyNCAPL5h7obAFAPwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+IKnP9HNjtrmdEOixiHAr83W057e9wvZB24dtP9nJugD0l9u9t9/2LEmHJD0gaVzSW5Iei4jfF5Zhzw/0WD/2/HdJOhwRRyPiz5J+IWllB+sD0EedhP96SSemvB+vpv0d2yO2x2yPdbAtAF3WyR/8pju0+MRhfUSMShqVOOwHBkkne/5xSQunvP+ipJOdtQOgXzoJ/1uSbrT9JduflfQNSdu70xaAXmv7sD8iLth+XNL/SJolaVNE7O9aZwB6qu1LfW1tjHN+oOf6cpMPgJmL8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaTaHqJbkmwfk3RO0seSLkTEUDeaAtB7HYW/cm9E/LEL6wHQRxz2A0l1Gv6QtMP2Htsj3WgIQH90eti/LCJO2p4v6RXb70XErqkzVL8U+MUADBhHRHdWZG+QdD4ivl+YpzsbA9BQRLiV+do+7Ld9te3PXXot6SuS9rW7PgD91clh/3WSfm370np+HhEvd6UrAD3XtcP+ljbGYT/Qcz0/7AcwsxF+ICnCDyRF+IGkCD+QFOEHkurGU30prFq1qmFtzZo1xWVPnjxZrH/00UfF+pYtW4r1999/v2Ht8OHDxWWRF3t+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iKR3pbdPTo0Ya1xYsX96+RaZw7d65hbf/+/X3sZLCMj483rD311FPFZcfGxrrdTt/wSC+AIsIPJEX4gaQIP5AU4QeSIvxAUoQfSIrn+VtUemb/tttuKy574MCBYv3mm28u1m+//fZifXh4uGHt7rvvLi574sSJYn3hwoXFeicuXLhQrJ89e7ZYX7BgQdvbPn78eLE+k6/zt4o9P5AU4QeSIvxAUoQfSIrwA0kRfiApwg8k1fR5ftubJH1V0pmIuLWaNk/SLyUtlnRM0qMR8UHTjc3g5/kH2dy5cxvWlixZUlx2z549xfqdd97ZVk+taDZewaFDh4r1ZvdPzJs3r2Ft7dq1xWU3btxYrA+ybj7P/zNJKy6b9qSknRFxo6Sd1XsAM0jT8EfELkkTl01eKWlz9XqzpIe73BeAHmv3nP+6iDglSdXP+d1rCUA/9PzeftsjkkZ6vR0AV6bdPf9p2wskqfp5ptGMETEaEUMRMdTmtgD0QLvh3y5pdfV6taTnu9MOgH5pGn7bz0p6Q9I/2R63/R+SvifpAdt/kPRA9R7ADML39mNgPfLII8X61q1bi/V9+/Y1rN17773FZScmLr/ANXPwvf0Aigg/kBThB5Ii/EBShB9IivADSXGpD7WZP7/8SMjevXs7Wn7VqlUNa9u2bSsuO5NxqQ9AEeEHkiL8QFKEH0iK8ANJEX4gKcIPJMUQ3ahNs6/Pvvbaa4v1Dz4of1v8wYMHr7inTNjzA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSPM+Pnlq2bFnD2quvvlpcdvbs2cX68PBwsb5r165i/dOK5/kBFBF+ICnCDyRF+IGkCD+QFOEHkiL8QFJNn+e3vUnSVyWdiYhbq2kbJK2RdLaabX1EvNSrJjFzPfjggw1rza7j79y5s1h/44032uoJk1rZ8/9M0opppv8oIpZU/wg+MMM0DX9E7JI00YdeAPRRJ+f8j9v+ne1Ntud2rSMAfdFu+DdK+rKkJZJOSfpBoxltj9gesz3W5rYA9EBb4Y+I0xHxcURclPQTSXcV5h2NiKGIGGq3SQDd11b4bS+Y8vZrkvZ1px0A/dLKpb5nJQ1L+rztcUnfkTRse4mkkHRM0jd72COAHuB5fnTkqquuKtZ3797dsHbLLbcUl73vvvuK9ddff71Yz4rn+QEUEX4gKcIPJEX4gaQIP5AU4QeSYohudGTdunXF+tKlSxvWXn755eKyXMrrLfb8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AUj/Si6KGHHirWn3vuuWL9ww8/bFhbsWK6L4X+mzfffLNYx/R4pBdAEeEHkiL8QFKEH0iK8ANJEX4gKcIPJMXz/Mldc801xfrTTz9drM+aNatYf+mlxgM4cx2/Xuz5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpps/z214o6RlJX5B0UdJoRPzY9jxJv5S0WNIxSY9GxAdN1sXz/H3W7Dp8s2vtd9xxR7F+5MiRYr30zH6zZdGebj7Pf0HStyPiZkl3S1pr+58lPSlpZ0TcKGln9R7ADNE0/BFxKiLerl6fk3RA0vWSVkraXM22WdLDvWoSQPdd0Tm/7cWSlkr6raTrIuKUNPkLQtL8bjcHoHdavrff9hxJ2yR9KyL+ZLd0WiHbI5JG2msPQK+0tOe3PVuTwd8SEb+qJp+2vaCqL5B0ZrplI2I0IoYiYqgbDQPojqbh9+Qu/qeSDkTED6eUtktaXb1eLen57rcHoFdaudS3XNJvJO3V5KU+SVqvyfP+rZIWSTou6esRMdFkXVzq67ObbrqpWH/vvfc6Wv/KlSuL9RdeeKGj9ePKtXqpr+k5f0TsltRoZfdfSVMABgd3+AFJEX4gKcIPJEX4gaQIP5AU4QeS4qu7PwVuuOGGhrUdO3Z0tO5169YV6y+++GJH60d92PMDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFJc5/8UGBlp/C1pixYt6mjdr732WrHe7PsgMLjY8wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUlznnwGWL19erD/xxBN96gSfJuz5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpptf5bS+U9IykL0i6KGk0In5se4OkNZLOVrOuj4iXetVoZvfcc0+xPmfOnLbXfeTIkWL9/Pnzba8bg62Vm3wuSPp2RLxt+3OS9th+par9KCK+37v2APRK0/BHxClJp6rX52wfkHR9rxsD0FtXdM5ve7GkpZJ+W0163PbvbG+yPbfBMiO2x2yPddQpgK5qOfy250jaJulbEfEnSRslfVnSEk0eGfxguuUiYjQihiJiqAv9AuiSlsJve7Ymg78lIn4lSRFxOiI+joiLkn4i6a7etQmg25qG37Yl/VTSgYj44ZTpC6bM9jVJ+7rfHoBeaeWv/csk/Zukvbbfqaatl/SY7SWSQtIxSd/sSYfoyLvvvlus33///cX6xMREN9vBAGnlr/27JXmaEtf0gRmMO/yApAg/kBThB5Ii/EBShB9IivADSbmfQyzbZjxnoMciYrpL85/Anh9IivADSRF+ICnCDyRF+IGkCD+QFOEHkur3EN1/lPR/U95/vpo2iAa1t0HtS6K3dnWztxtanbGvN/l8YuP22KB+t9+g9jaofUn01q66euOwH0iK8ANJ1R3+0Zq3XzKovQ1qXxK9tauW3mo95wdQn7r3/ABqUkv4ba+wfdD2YdtP1tFDI7aP2d5r+526hxirhkE7Y3vflGnzbL9i+w/Vz2mHSauptw22/7/67N6x/WBNvS20/b+2D9jeb/s/q+m1fnaFvmr53Pp+2G97lqRDkh6QNC7pLUmPRcTv+9pIA7aPSRqKiNqvCdv+F0nnJT0TEbdW056SNBER36t+cc6NiP8akN42SDpf98jN1YAyC6aOLC3pYUn/rho/u0Jfj6qGz62OPf9dkg5HxNGI+LOkX0haWUMfAy8idkm6fNSMlZI2V683a/I/T9816G0gRMSpiHi7en1O0qWRpWv97Ap91aKO8F8v6cSU9+MarCG/Q9IO23tsj9TdzDSuq4ZNvzR8+vya+7lc05Gb++mykaUH5rNrZ8Trbqsj/NN9xdAgXXJYFhG3S/pXSWurw1u0pqWRm/tlmpGlB0K7I153Wx3hH5e0cMr7L0o6WUMf04qIk9XPM5J+rcEbffj0pUFSq59nau7nrwZp5ObpRpbWAHx2gzTidR3hf0vSjba/ZPuzkr4haXsNfXyC7aurP8TI9tWSvqLBG314u6TV1evVkp6vsZe/MygjNzcaWVo1f3aDNuJ1LTf5VJcy/lvSLEmbIuK7fW9iGrb/UZN7e2nyicef19mb7WclDWvyqa/Tkr4j6TlJWyUtknRc0tcjou9/eGvQ27AmD13/OnLzpXPsPve2XNJvJO2VdLGavF6T59e1fXaFvh5TDZ8bd/gBSXGHH5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpP4CIJjqosJxHysAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_dataset[0][0].numpy().reshape(28,28),'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "n_iters=3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs=n_iters/(len(train_dataset)/batch_size)\n",
    "num_epochs=int(num_epochs)\n",
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "isinstance(train_loader,collections.Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear=nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=28*28\n",
    "output_dim=10\n",
    "model=LogisticRegression(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 loss 0.8899600505828857 accuracy83\n",
      "iteration 1000 loss 0.8486135601997375 accuracy83\n",
      "iteration 1500 loss 0.8319770693778992 accuracy84\n",
      "iteration 2000 loss 0.7919811010360718 accuracy84\n",
      "iteration 2500 loss 0.7475183010101318 accuracy85\n",
      "iteration 3000 loss 0.813260555267334 accuracy85\n"
     ]
    }
   ],
   "source": [
    "iter=0\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images=Variable(images.view(-1,28*28))\n",
    "        labels=Variable(labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs=model(images)\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter+=1\n",
    "        \n",
    "        if iter%500==0:\n",
    "            correct=0\n",
    "            total=0\n",
    "            \n",
    "            for images,labels in test_loader:\n",
    "                images=Variable(images.view(-1,28*28))\n",
    "                \n",
    "                outputs=model(images)\n",
    "                \n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                \n",
    "                total+=labels.size(0)\n",
    "                correct+=(predicted==labels).sum()\n",
    "                \n",
    "            accuracy=100*correct/total\n",
    "                \n",
    "                \n",
    "            print('iteration {} loss {} accuracy{}'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs\n",
      "<built-in method size of Tensor object at 0x000000C08F76AB40>\n"
     ]
    }
   ],
   "source": [
    "iter_test=0\n",
    "for images,labels in test_loader:\n",
    "    iter_test+=1\n",
    "    images=Variable(images.view(-1,28*28))\n",
    "    outputs=model(images)\n",
    "    \n",
    "    if iter_test==1:\n",
    "        print('outputs')\n",
    "        print(outputs.size)\n",
    "    _,predicted=torch.max(outputs.data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs\n",
      "tensor([-0.3497, -1.7511, -0.5945,  0.1088, -0.0715, -0.4993, -1.3385,  4.1155,\n",
      "        -0.2416,  1.4293], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "iter_test=0\n",
    "for images,labels in test_loader:\n",
    "    iter_test+=1\n",
    "    images=Variable(images.view(-1,28*28))\n",
    "    outputs=model(images)\n",
    "    \n",
    "    if iter_test==1:\n",
    "        print('outputs')\n",
    "        print(outputs[0,:])\n",
    "    _,predicted=torch.max(outputs.data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "tensor(7)\n",
      "label size\n",
      "torch.Size([100])\n",
      "label for image 0\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "iter_test=0\n",
    "for images,labels in test_loader:\n",
    "    iter_test+=1\n",
    "    images=Variable(images.view(-1,28*28))\n",
    "    outputs=model(images)\n",
    "    _,predicted=torch.max(outputs.data,1)\n",
    "    \n",
    "    if iter_test==1:\n",
    "        print('Prediction')\n",
    "        print(predicted[0])\n",
    "        \n",
    "        print('label size')\n",
    "        print(labels.size())\n",
    "        \n",
    "        print('label for image 0')\n",
    "        print(labels[0])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeedForward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=dsets.MNIST(root='./data',train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "test_dataset=dsets.MNIST(root='./data',train=False,\n",
    "                         transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "n_iters=3000\n",
    "num_epochs=n_iters/(len(train_dataset)/batch_size)\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedforwardnn(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(feedforwardnn,self).__init__()\n",
    "        \n",
    "        self.fc1=nn.Linear(input_dim,hidden_dim)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.fc2=nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.fc1(x)\n",
    "        \n",
    "        out=self.sigmoid(out)\n",
    "        \n",
    "        out=self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=28*28\n",
    "hidden_dim=100\n",
    "output_dim=10\n",
    "\n",
    "model=feedforwardnn(input_dim,hidden_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x00000034E871CBF8>\n",
      "4\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "print(list(model.parameters())[0].size())\n",
    "print(list(model.parameters())[1].size())\n",
    "print(list(model.parameters())[2].size())\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 loss 0.5552537441253662 accuracy85\n",
      "iteration 1000 loss 0.3868594765663147 accuracy89\n",
      "iteration 1500 loss 0.3034842610359192 accuracy90\n",
      "iteration 2000 loss 0.2385861724615097 accuracy91\n",
      "iteration 2500 loss 0.2963238060474396 accuracy91\n",
      "iteration 3000 loss 0.2925407290458679 accuracy92\n"
     ]
    }
   ],
   "source": [
    "iter=0\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images=Variable(images.view(-1,28*28))\n",
    "        labels=Variable(labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs=model(images)\n",
    "        loss=criteria(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter+=1\n",
    "        \n",
    "        if iter%500==0:\n",
    "            correct=0\n",
    "            total=0\n",
    "            \n",
    "            for images,labels in test_loader:\n",
    "                images=Variable(images.view(-1,28*28))\n",
    "                \n",
    "                outputs=model(images)\n",
    "                \n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                \n",
    "                total+=labels.size(0)\n",
    "                correct+=(predicted==labels).sum()\n",
    "                \n",
    "            accuracy=100*correct/total\n",
    "                \n",
    "                \n",
    "            print('iteration {} loss {} accuracy{}'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=dsets.MNIST(root='./data',\n",
    "                         train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "test_dataset=dsets.MNIST(root='./data',train=False,\n",
    "                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "n_iters=3000\n",
    "\n",
    "num_epochs=n_iters/(len(train_dataset)/batch_size)\n",
    "num_epochs=int(num_epochs)\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnnModel,self).__init__()\n",
    "        \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=16,\n",
    "                           kernel_size=5,stride=1,padding=2)\n",
    "        \n",
    "        self.relu1=nn.ReLU()\n",
    "        \n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.cnn2=nn.Conv2d(in_channels=16,out_channels=32,\n",
    "                           kernel_size=5,stride=1,padding=2)\n",
    "        \n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.fc1=nn.Linear(32*7*7,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out=self.cnn1(x)\n",
    "        out=self.relu1(out)\n",
    "        \n",
    "        out=self.maxpool1(out)\n",
    "        \n",
    "        out=self.cnn2(out)\n",
    "        out=self.relu2(out)\n",
    "        \n",
    "        out=self.maxpool2(out)\n",
    "        \n",
    "        out=out.view(out.size(0),-1)\n",
    "        \n",
    "        out=self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria=nn.CrossEntropyLoss()\n",
    "learning_rate=0.01\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 loss 0.31424111127853394 accuracy89\n",
      "iteration 1000 loss 0.20720337331295013 accuracy93\n",
      "iteration 1500 loss 0.34727370738983154 accuracy95\n",
      "iteration 2000 loss 0.15701313316822052 accuracy95\n",
      "iteration 2500 loss 0.22879895567893982 accuracy96\n",
      "iteration 3000 loss 0.08581600338220596 accuracy96\n"
     ]
    }
   ],
   "source": [
    "iter=0\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images=Variable(images)\n",
    "        labels=Variable(labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs=model(images)\n",
    "        loss=criteria(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter+=1\n",
    "        \n",
    "        if iter%500==0:\n",
    "            correct=0\n",
    "            total=0\n",
    "            \n",
    "            for images,labels in test_loader:\n",
    "                images=Variable(images)\n",
    "                \n",
    "                outputs=model(images)\n",
    "                \n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                \n",
    "                total+=labels.size(0)\n",
    "                correct+=(predicted==labels).sum()\n",
    "                \n",
    "            accuracy=100*correct/total\n",
    "                \n",
    "                \n",
    "            print('iteration {} loss {} accuracy{}'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=dsets.MNIST(root='./data',train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "test_dataset=dsets.MNIST(root='./data',\n",
    "                        train=False,\n",
    "                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "n_iters=3000\n",
    "num_epochs=n_iters/(len(train_dataset)/batch_size)\n",
    "num_epochs=int(num_epochs)\n",
    "\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnnModel(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,layer_dim,output_dim):\n",
    "        super(rnnModel,self).__init__()\n",
    "        \n",
    "        self.hidden_dim=hidden_dim\n",
    "        \n",
    "        self.layer_dim=layer_dim\n",
    "        \n",
    "        self.rnn=nn.RNN(input_dim,hidden_dim,layer_dim,batch_first=True,nonlinearity='relu')\n",
    "        \n",
    "        self.fc=nn.Linear(hidden_dim,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        h0=Variable(torch.zeros(self.layer_dim,x.size(0),self.hidden_dim))\n",
    "               \n",
    "        out,hn=self.rnn(x,h0)\n",
    "        out=self.fc(out[:,-1,:])\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=28\n",
    "hidden_dim=100\n",
    "layer_dim=1\n",
    "output_dim=10\n",
    "\n",
    "model=rnnModel(input_dim,hidden_dim,layer_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 loss 1.3023524284362793 accuracy55\n",
      "iteration 1000 loss 0.7688631415367126 accuracy59\n",
      "iteration 1500 loss 0.9617370367050171 accuracy67\n",
      "iteration 2000 loss 1.0255221128463745 accuracy61\n",
      "iteration 2500 loss 0.6692538261413574 accuracy68\n",
      "iteration 3000 loss 0.7344135046005249 accuracy88\n"
     ]
    }
   ],
   "source": [
    "seq_dim=28\n",
    "iter=0\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images=Variable(images.view(-1,seq_dim,input_dim))\n",
    "        labels=Variable(labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs=model(images)\n",
    "        loss=criteria(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter+=1\n",
    "        \n",
    "        if iter%500==0:\n",
    "            correct=0\n",
    "            total=0\n",
    "            \n",
    "            for images,labels in test_loader:\n",
    "                images=Variable(images.view(-1,seq_dim,input_dim))\n",
    "                \n",
    "                outputs=model(images)\n",
    "                \n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                \n",
    "                total+=labels.size(0)\n",
    "                correct+=(predicted==labels).sum()\n",
    "                \n",
    "            accuracy=100*correct/total\n",
    "                \n",
    "                \n",
    "            print('iteration {} loss {} accuracy{}'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN - 2 Reference - https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=dsets.MNIST(root='./data',\n",
    "                         train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=dsets.MNIST(root='./data',\n",
    "                        train=False,\n",
    "                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=100,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=100,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer1=nn.Sequential(\n",
    "        nn.Conv2d(1,16,kernel_size=5,stride=1,padding=2),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.layer2=nn.Sequential(\n",
    "        nn.Conv2d(16,32,kernel_size=5,stride=1,padding=2),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "        self.fc=nn.Linear(7*7*32,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out=self.layer1(x)\n",
    "        out=self.layer2(out)\n",
    "        out=out.reshape(out.size(0),-1)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/600], Loss: 0.0525\n",
      "Epoch [1/3], Step [200/600], Loss: 0.0450\n",
      "Epoch [1/3], Step [300/600], Loss: 0.0932\n",
      "Epoch [1/3], Step [400/600], Loss: 0.0897\n",
      "Epoch [1/3], Step [500/600], Loss: 0.0335\n",
      "Epoch [1/3], Step [600/600], Loss: 0.0357\n",
      "Epoch [2/3], Step [100/600], Loss: 0.0349\n",
      "Epoch [2/3], Step [200/600], Loss: 0.0159\n",
      "Epoch [2/3], Step [300/600], Loss: 0.0103\n",
      "Epoch [2/3], Step [400/600], Loss: 0.0299\n",
      "Epoch [2/3], Step [500/600], Loss: 0.0303\n",
      "Epoch [2/3], Step [600/600], Loss: 0.0277\n",
      "Epoch [3/3], Step [100/600], Loss: 0.0629\n",
      "Epoch [3/3], Step [200/600], Loss: 0.0451\n",
      "Epoch [3/3], Step [300/600], Loss: 0.0343\n",
      "Epoch [3/3], Step [400/600], Loss: 0.0305\n",
      "Epoch [3/3], Step [500/600], Loss: 0.0577\n",
      "Epoch [3/3], Step [600/600], Loss: 0.0690\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(3):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, 3, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.86 %\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
